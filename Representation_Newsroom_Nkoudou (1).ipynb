{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c08d2cd-8103-496f-9375-a54ece82a021",
   "metadata": {},
   "source": [
    "# EPS711: Text and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f9f32-bd77-491c-9e8c-1933a7c80a18",
   "metadata": {},
   "source": [
    "## Week 3: Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed40fa-d52d-46ac-b86a-0664975d0696",
   "metadata": {},
   "source": [
    "To start this week's assignment, run below and load the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53627f9f-b3db-4cbf-a270-dd1ec940f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame with all articles:\n",
      "                                             content  category  \\\n",
      "0  I was wondering if anyone out there could enli...         7   \n",
      "1  A fair number of brave souls who upgraded thei...         4   \n",
      "2  well folks, my mac plus finally gave up the gh...         4   \n",
      "3  \\nDo you have Weitek's address/phone number?  ...         1   \n",
      "4  From article <C5owCB.n3p@world.std.com>, by to...        14   \n",
      "\n",
      "           category_name  \n",
      "0              rec.autos  \n",
      "1  comp.sys.mac.hardware  \n",
      "2  comp.sys.mac.hardware  \n",
      "3          comp.graphics  \n",
      "4              sci.space  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Load the full 20 Newsgroups dataset from sklearn\n",
    "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a pandas DataFrame from the dataset\n",
    "df = pd.DataFrame({'content': newsgroups.data, 'category': newsgroups.target})\n",
    "\n",
    "# Add category names for easier understanding\n",
    "df['category_name'] = df['category'].apply(lambda x: newsgroups.target_names[x])\n",
    "\n",
    "# Display the initial DataFrame\n",
    "print(\"Initial DataFrame with all articles:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a3c049-bc04-4923-9f6c-9964208e8527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with 50 articles per category:\n",
      "                                             content  category category_name\n",
      "0  #>So instead of calling it interest on deposit...         0   alt.atheism\n",
      "1  \\n  Actually, my atheism is based on ignorance...         0   alt.atheism\n",
      "2  \\nProbably we would have much the same problem...         0   alt.atheism\n",
      "3  frank@D012S658.uucp (Frank O'Dwyer) writes ......         0   alt.atheism\n",
      "4  \\nDidn't you hear?  His address has changed.  ...         0   alt.atheism\n",
      "                                               content  category  \\\n",
      "995  \\n#In <mcclaryC5snpq.KB1@netcom.com> mcclary@n...        19   \n",
      "996  Walter-\\n\\nI tried several times in the past t...        19   \n",
      "997  \\nCan we assume from this statement that you a...        19   \n",
      "998  \\nJim, please, that's a lame explanation of th...        19   \n",
      "999  PSA 145:9  The LORD is good to all: and his  t...        19   \n",
      "\n",
      "          category_name  \n",
      "995  talk.religion.misc  \n",
      "996  talk.religion.misc  \n",
      "997  talk.religion.misc  \n",
      "998  talk.religion.misc  \n",
      "999  talk.religion.misc  \n",
      "\n",
      "Number of articles in each category:\n",
      "category_name\n",
      "alt.atheism                 50\n",
      "comp.graphics               50\n",
      "talk.politics.misc          50\n",
      "talk.politics.mideast       50\n",
      "talk.politics.guns          50\n",
      "soc.religion.christian      50\n",
      "sci.space                   50\n",
      "sci.med                     50\n",
      "sci.electronics             50\n",
      "sci.crypt                   50\n",
      "rec.sport.hockey            50\n",
      "rec.sport.baseball          50\n",
      "rec.motorcycles             50\n",
      "rec.autos                   50\n",
      "misc.forsale                50\n",
      "comp.windows.x              50\n",
      "comp.sys.mac.hardware       50\n",
      "comp.sys.ibm.pc.hardware    50\n",
      "comp.os.ms-windows.misc     50\n",
      "talk.religion.misc          50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample 50 articles per category\n",
    "df_subset = pd.concat(\n",
    "    [group.sample(n=50, random_state=12) for _, group in df.groupby('category')]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Display the subset DataFrame\n",
    "print(\"\\nDataFrame with 50 articles per category:\")\n",
    "print(df_subset.head())\n",
    "print(df_subset.tail())\n",
    "\n",
    "print(\"\\nNumber of articles in each category:\")\n",
    "print(df_subset['category_name'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a070ee9f-d162-45c4-b060-c3ca677d885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  category category_name\n",
      "0  #>So instead of calling it interest on deposit...         0   alt.atheism\n",
      "1  \\n  Actually, my atheism is based on ignorance...         0   alt.atheism\n",
      "category_name\n",
      "alt.atheism                 50\n",
      "comp.graphics               50\n",
      "talk.politics.misc          50\n",
      "talk.politics.mideast       50\n",
      "talk.politics.guns          50\n",
      "soc.religion.christian      50\n",
      "sci.space                   50\n",
      "sci.med                     50\n",
      "sci.electronics             50\n",
      "sci.crypt                   50\n",
      "rec.sport.hockey            50\n",
      "rec.sport.baseball          50\n",
      "rec.motorcycles             50\n",
      "rec.autos                   50\n",
      "misc.forsale                50\n",
      "comp.windows.x              50\n",
      "comp.sys.mac.hardware       50\n",
      "comp.sys.ibm.pc.hardware    50\n",
      "comp.os.ms-windows.misc     50\n",
      "talk.religion.misc          50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group by 'category' and sample 50 articles from each category\n",
    "### df_subset = df.groupby('category').apply(lambda x: x.sample(n=50, random_state=12), include_groups=False).reset_index(drop=True) \n",
    "### the line above causes the following error due to \"include_groups=False\": /var/folders/nv/37z9hwt54flbq13cryjqrzm40000gn/T/ipykernel_60088/1030170394.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
    "### alternative 1: \n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load dataset\n",
    "newsgroups = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
    "df = pd.DataFrame({\n",
    "    'content': newsgroups.data,\n",
    "    'category': newsgroups.target\n",
    "})\n",
    "df['category_name'] = df['category'].apply(lambda i: newsgroups.target_names[i])\n",
    "\n",
    "# Build df_subset with 50 samples per category (stable across pandas versions)\n",
    "df_subset = pd.concat(\n",
    "    [g.sample(n=50, random_state=12) for _, g in df.groupby('category')]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Sanity checks\n",
    "print(df_subset.head(2))\n",
    "print(df_subset['category_name'].value_counts())\n",
    "### alternative 2:\n",
    "### df_subset = (df.groupby('category', group_keys=False).apply(lambda x: x.sample(n=50, random_state=12)) .reset_index(drop=True)\n",
    "# Prevents inclusion of grouping columns\n",
    " # Sample 50 rows from each group\n",
    "### warning message again: /var/folders/nv/37z9hwt54flbq13cryjqrzm40000gn/T/ipykernel_60088/1243261900.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
    "# apply(lambda x: x.sample(n=50, random_state=12))  \n",
    "### alternative 3: \n",
    "### df_subset = (df.groupby('category').apply(lambda x: x.sample(n=50, random_state=12)[['content', 'category', 'category_name']]).reset_index(drop=True))\n",
    "\n",
    "### alternative 4: which works: \n",
    "#df_subset = (pd.concat([group.sample(n=50, random_state=12) for _, group in df.groupby('category')]).reset_index(drop=True))\n",
    "\n",
    "###/var/folders/nv/37z9hwt54flbq13cryjqrzm40000gn/T/ipykernel_60088/2251538078.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning. .apply(lambda x: x.sample(n=50, random_state=12)[['content', 'category', 'category_name']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c5731-c14a-459d-88fe-9a9dd45009ff",
   "metadata": {},
   "source": [
    "The dataset above is borrowed from one of the sklearn datasets. You can learn more about the dataset from this document: [https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset). \n",
    "\n",
    "There are 20 categories of news articles in the dataset. Since the dataset is huge, I loaded only 50 articles per each category, so you have 1,000 articles to work with in total. I have already loaded the data in the dataframe `df_subset[]`, and I will create filenames for easier reference and also see how long each article is as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e035ffdf-1508-40b6-91a3-07e16befb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with filenames added:\n",
      "                                             content  category category_name  \\\n",
      "0  #>So instead of calling it interest on deposit...         0   alt.atheism   \n",
      "1  \\n  Actually, my atheism is based on ignorance...         0   alt.atheism   \n",
      "2  \\nProbably we would have much the same problem...         0   alt.atheism   \n",
      "3  frank@D012S658.uucp (Frank O'Dwyer) writes ......         0   alt.atheism   \n",
      "4  \\nDidn't you hear?  His address has changed.  ...         0   alt.atheism   \n",
      "\n",
      "        filename  content_length  \n",
      "0  alt.atheism_0            1412  \n",
      "1  alt.atheism_1             372  \n",
      "2  alt.atheism_2            1359  \n",
      "3  alt.atheism_3            1893  \n",
      "4  alt.atheism_4             191  \n",
      "                                               content  category  \\\n",
      "995  \\n#In <mcclaryC5snpq.KB1@netcom.com> mcclary@n...        19   \n",
      "996  Walter-\\n\\nI tried several times in the past t...        19   \n",
      "997  \\nCan we assume from this statement that you a...        19   \n",
      "998  \\nJim, please, that's a lame explanation of th...        19   \n",
      "999  PSA 145:9  The LORD is good to all: and his  t...        19   \n",
      "\n",
      "          category_name               filename  content_length  \n",
      "995  talk.religion.misc  talk.religion.misc_45            8621  \n",
      "996  talk.religion.misc  talk.religion.misc_46             492  \n",
      "997  talk.religion.misc  talk.religion.misc_47             288  \n",
      "998  talk.religion.misc  talk.religion.misc_48             266  \n",
      "999  talk.religion.misc  talk.religion.misc_49              85  \n"
     ]
    }
   ],
   "source": [
    "# Add filenames to the DataFrame for easier reference\n",
    "# Include category name in the filename for better identification\n",
    "# Add content length for analysis purposes\n",
    "df_subset['filename'] = df_subset.groupby('category_name').cumcount().astype(str).radd(df_subset['category_name'] + '_')\n",
    "df_subset['content_length'] = df_subset['content'].apply(len)\n",
    "\n",
    "# Display the updated DataFrame with filenames\n",
    "print(\"\\nDataFrame with filenames added:\")\n",
    "print(df_subset.head())\n",
    "print(df_subset.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358623a6-b24f-4789-afac-a03f803e425f",
   "metadata": {},
   "source": [
    "### Problem 1. Preprocess the data content and display the preprocessed data in the dataframe with the original data.\n",
    "This subset of 50 articles per category is now ready for further text representation tasks. Let's first do preprocessing by removing unncessary symbols and stop words, lowercasing the letter, and tokenizing. Also, print the results of the preprocessing in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6a26284-9819-4c84-b23c-6b28f9abe494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_name</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>so instead of calling it interest on deposits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>actually my atheism is based on ignorance igno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>probably we would have much the same problems ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>frank d s uucp frank o'dwyer writes while i'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>didn't you hear his address has changed he can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>if a person gives a well balanced reasoned arg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>so that still leaves the door totally open for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>last night while watching the a m rebroadcast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>most of post deleted there is an easy way out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>the quotation marks should enclose laws not mu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category_name                                              clean\n",
       "0   alt.atheism  so instead of calling it interest on deposits ...\n",
       "1   alt.atheism  actually my atheism is based on ignorance igno...\n",
       "2   alt.atheism  probably we would have much the same problems ...\n",
       "3   alt.atheism  frank d s uucp frank o'dwyer writes while i'll...\n",
       "4   alt.atheism  didn't you hear his address has changed he can...\n",
       "5   alt.atheism  if a person gives a well balanced reasoned arg...\n",
       "6   alt.atheism  so that still leaves the door totally open for...\n",
       "7   alt.atheism  last night while watching the a m rebroadcast ...\n",
       "8   alt.atheism  most of post deleted there is an easy way out ...\n",
       "9   alt.atheism  the quotation marks should enclose laws not mu..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your code here\n",
    "import re\n",
    "# Simple preprocessing:\n",
    "# - lowercase\n",
    "# - remove URLs\n",
    "# - keep only letters/spaces/apostrophes\n",
    "# - collapse multiple spaces\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s or \"\"\n",
    "    s = s.lower()\n",
    "    s = url_pattern.sub(' ', s)\n",
    "    s = re.sub(r\"[^a-z\\s']\", \" \", s)   # keep letters, spaces, apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df_subset['clean'] = df_subset['content'].apply(clean_text)\n",
    "\n",
    "# Show preview\n",
    "df_subset[['category_name', 'clean']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564682a8-9ae4-423d-bc42-7c062b7b08e1",
   "metadata": {},
   "source": [
    "### Problem 2. Create a bag of words using `CountVectorizer` and print the results in the form of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f54c065-289d-45ea-9298-d789c7aac943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aan</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abiding</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zelepukin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zijn</th>\n",
       "      <th>zip</th>\n",
       "      <th>zjp</th>\n",
       "      <th>zo</th>\n",
       "      <th>zone</th>\n",
       "      <th>zorg</th>\n",
       "      <th>zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aan  abandoned  abc  abiding  ability  able  absolute  absolutely  \\\n",
       "0   0    0          0    0        0        0     0         0           0   \n",
       "1   0    0          0    0        0        0     0         0           0   \n",
       "2   0    0          0    0        0        0     0         0           0   \n",
       "3   0    0          0    0        0        0     0         3           0   \n",
       "4   0    0          0    0        0        0     0         0           0   \n",
       "5   0    0          0    0        0        0     0         0           0   \n",
       "6   0    0          0    0        0        0     0         0           0   \n",
       "7   0    0          0    0        0        0     0         0           0   \n",
       "8   0    0          0    0        0        0     0         0           0   \n",
       "9   0    0          0    0        0        0     0         0           0   \n",
       "\n",
       "   abstract  ...  zealand  zelepukin  zero  zijn  zip  zjp  zo  zone  zorg  zs  \n",
       "0         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "1         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "2         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "3         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "4         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "5         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "6         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "7         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "8         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "9         0  ...        0          0     0     0    0    0   0     0     0   0  \n",
       "\n",
       "[10 rows x 5000 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your code from \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "bow_vect = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit and transform the clean text\n",
    "X_bow = bow_vect.fit_transform(df_subset['clean'])\n",
    "\n",
    "# Convert to DataFrame for easier inspection\n",
    "bow_df = pd.DataFrame(X_bow.toarray(), columns=bow_vect.get_feature_names_out())\n",
    "\n",
    "# Show first 10 rows\n",
    "bow_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31669deb-610e-4d7b-a408-ece173b0b84e",
   "metadata": {},
   "source": [
    "### Problem 3. Create bigrams using `CountVectorizer` and print the results in the form of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5daedf0-c299-4891-84ac-3075a51b14ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able run</th>\n",
       "      <th>ac uk</th>\n",
       "      <th>alt atheism</th>\n",
       "      <th>anonymous ftp</th>\n",
       "      <th>answer question</th>\n",
       "      <th>anybody know</th>\n",
       "      <th>appreciated thanks</th>\n",
       "      <th>ask questions</th>\n",
       "      <th>available ftp</th>\n",
       "      <th>berkeley edu</th>\n",
       "      <th>...</th>\n",
       "      <th>video card</th>\n",
       "      <th>want know</th>\n",
       "      <th>want use</th>\n",
       "      <th>washington dc</th>\n",
       "      <th>weeks ago</th>\n",
       "      <th>window manager</th>\n",
       "      <th>windows dos</th>\n",
       "      <th>works fine</th>\n",
       "      <th>year old</th>\n",
       "      <th>years ago</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able run  ac uk  alt atheism  anonymous ftp  answer question  anybody know  \\\n",
       "0         0      0            0              0                0             0   \n",
       "1         0      0            0              0                0             0   \n",
       "2         0      0            0              0                0             0   \n",
       "3         0      0            0              0                0             0   \n",
       "4         0      0            0              0                0             0   \n",
       "5         0      0            0              0                0             0   \n",
       "6         0      0            0              0                0             0   \n",
       "7         0      0            0              0                0             0   \n",
       "8         0      0            0              0                0             0   \n",
       "9         0      0            0              0                0             0   \n",
       "\n",
       "   appreciated thanks  ask questions  available ftp  berkeley edu  ...  \\\n",
       "0                   0              0              0             0  ...   \n",
       "1                   0              0              0             0  ...   \n",
       "2                   0              0              0             0  ...   \n",
       "3                   0              0              0             0  ...   \n",
       "4                   0              0              0             0  ...   \n",
       "5                   0              0              0             0  ...   \n",
       "6                   0              0              0             0  ...   \n",
       "7                   0              0              0             0  ...   \n",
       "8                   0              0              0             0  ...   \n",
       "9                   0              0              0             0  ...   \n",
       "\n",
       "   video card  want know  want use  washington dc  weeks ago  window manager  \\\n",
       "0           0          0         0              0          0               0   \n",
       "1           0          0         0              0          0               0   \n",
       "2           0          0         0              0          0               0   \n",
       "3           0          0         0              0          0               0   \n",
       "4           0          0         0              0          0               0   \n",
       "5           0          0         0              0          0               0   \n",
       "6           0          0         0              0          0               0   \n",
       "7           0          0         0              0          0               0   \n",
       "8           0          0         0              0          0               0   \n",
       "9           0          0         0              0          0               0   \n",
       "\n",
       "   windows dos  works fine  year old  years ago  \n",
       "0            0           0         0          0  \n",
       "1            0           0         0          0  \n",
       "2            0           0         0          0  \n",
       "3            0           0         0          0  \n",
       "4            0           0         0          0  \n",
       "5            0           0         0          0  \n",
       "6            0           0         0          0  \n",
       "7            0           0         1          0  \n",
       "8            0           0         0          0  \n",
       "9            0           0         0          0  \n",
       "\n",
       "[10 rows x 127 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your code \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize CountVectorizer for bigrams\n",
    "bigram_vect = CountVectorizer(stop_words='english',\n",
    "                              ngram_range=(2, 2),   # (min_n, max_n) = only bigrams\n",
    "                              max_features=5000,    # limit vocabulary size\n",
    "                              min_df=5)             # only keep bigrams that appear in ≥5 docs\n",
    "\n",
    "# Fit and transform the clean text\n",
    "X_bigrams = bigram_vect.fit_transform(df_subset['clean'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "bigram_df = pd.DataFrame(X_bigrams.toarray(), columns=bigram_vect.get_feature_names_out())\n",
    "\n",
    "# Show first 10 rows\n",
    "bigram_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b845d2-f146-4891-916b-f0b9c5a15791",
   "metadata": {},
   "source": [
    "### Problem 4-1. Create TF-IDF Representation using `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "534a8845-2039-4914-b0e5-7d993f6bc813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix shape: (1000, 5000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aan</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abiding</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zelepukin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zijn</th>\n",
       "      <th>zip</th>\n",
       "      <th>zjp</th>\n",
       "      <th>zo</th>\n",
       "      <th>zone</th>\n",
       "      <th>zorg</th>\n",
       "      <th>zs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aan  abandoned  abc  abiding  ability  able  absolute  absolutely  \\\n",
       "0  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.000000         0.0   \n",
       "1  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.000000         0.0   \n",
       "2  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.000000         0.0   \n",
       "3  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.222638         0.0   \n",
       "4  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.000000         0.0   \n",
       "5  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.000000         0.0   \n",
       "6  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.000000         0.0   \n",
       "7  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.000000         0.0   \n",
       "8  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.000000         0.0   \n",
       "9  0.0  0.0        0.0  0.0      0.0      0.0   0.0  0.000000         0.0   \n",
       "\n",
       "   abstract  ...  zealand  zelepukin  zero  zijn  zip  zjp   zo  zone  zorg  \\\n",
       "0       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "1       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "2       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "3       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "4       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "5       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "6       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "7       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "8       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "9       0.0  ...      0.0        0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   \n",
       "\n",
       "    zs  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  \n",
       "5  0.0  \n",
       "6  0.0  \n",
       "7  0.0  \n",
       "8  0.0  \n",
       "9  0.0  \n",
       "\n",
       "[10 rows x 5000 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your code here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit and transform the clean text\n",
    "X_tfidf = tfidf_vect.fit_transform(df_subset['clean'])\n",
    "\n",
    "# Show the shape: (#documents, #features)\n",
    "print(\"TF-IDF Matrix shape:\", X_tfidf.shape)\n",
    "\n",
    "# Convert to DataFrame for easier inspection (optional — large!)\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vect.get_feature_names_out())\n",
    "\n",
    "# Show first 10 rows\n",
    "tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614342c-ad01-4931-b3b3-c4873645ec59",
   "metadata": {},
   "source": [
    "### Problem 4-2. Display top 10 words from 10 random, different articles based on the TF-IDF results, and write down your explanation in your language. What are the 10 words and what does each TF-IDF score mean? Also, make a guess about what each document is about based on the TF-IDF scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc6d7ab1-8636-4709-b37c-34c2701c5953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Document 1  (index 85)  |  Category: comp.graphics ===\n",
      "Preview:  ...for very small values of six and nine....\n",
      "Top 10 TF-IDF terms:\n",
      "  values               0.8008\n",
      "  small                0.5989\n",
      "Topic guess: values | small\n",
      "\n",
      "=== Document 2  (index 767)  |  Category: soc.religion.christian ===\n",
      "Preview: : I may be wrong, but wasn't Jeff Fenholt part of Black Sabbath?  He's a : MAJOR brother in Christ now.  He totally chan...\n",
      "Top 10 TF-IDF terms:\n",
      "  sabbath              0.4808\n",
      "  black                0.3492\n",
      "  jeff                 0.3301\n",
      "  christ               0.2525\n",
      "  wasn                 0.2345\n",
      "  wrong                0.1973\n",
      "  bands                0.1709\n",
      "  band                 0.1651\n",
      "  witnessing           0.1651\n",
      "  listening            0.1562\n",
      "Topic guess: sabbath | black | jeff | christ | wasn\n",
      "\n",
      "=== Document 3  (index 88)  |  Category: comp.graphics ===\n",
      "Preview: Organization: \"A World of Information at your Fingertips\" Keywords:     Craig,  You should still consider the Targa+. I ...\n",
      "Top 10 TF-IDF terms:\n",
      "  craig                0.3925\n",
      "  organization         0.3186\n",
      "  driver               0.2851\n",
      "  fine                 0.2851\n",
      "  consider             0.2758\n",
      "  works                0.2647\n",
      "  windows              0.2552\n",
      "  run                  0.2492\n",
      "  information          0.2416\n",
      "  world                0.2406\n",
      "Topic guess: craig | organization | driver | fine | consider\n",
      "\n",
      "=== Document 4  (index 649)  |  Category: sci.electronics ===\n",
      "Preview: The object of a cooling tower is to distribute dissolved salts in  cooling water over large areas of farmland and to the...\n",
      "Top 10 TF-IDF terms:\n",
      "  cooling              0.4944\n",
      "  farm                 0.2472\n",
      "  rendering            0.2388\n",
      "  deficit              0.2388\n",
      "  reduction            0.2388\n",
      "  industrial           0.2318\n",
      "  processes            0.2318\n",
      "  tower                0.2260\n",
      "  object               0.2125\n",
      "  water                0.2056\n",
      "Topic guess: cooling | farm | rendering | deficit | reduction\n",
      "\n",
      "=== Document 5  (index 436)  |  Category: rec.motorcycles ===\n",
      "Preview:  \tDo I have to be the one to say it?  \tDON'T BE SO STUPID AS TO LEAVE YOUR HELMET ON THE SEAT WHERE IT CAN \tFALL DOWN AN...\n",
      "Top 10 TF-IDF terms:\n",
      "  helmet               0.7212\n",
      "  protect              0.2840\n",
      "  fall                 0.2689\n",
      "  replace              0.1970\n",
      "  leave                0.1793\n",
      "  chair                0.1255\n",
      "  ers                  0.1161\n",
      "  landed               0.1127\n",
      "  rec                  0.1127\n",
      "  conservative         0.1099\n",
      "Topic guess: helmet | protect | fall | replace | leave\n",
      "\n",
      "=== Document 6  (index 430)  |  Category: rec.motorcycles ===\n",
      "Preview:  Oh! For a second I thought this was a posting by Ed Green!...\n",
      "Top 10 TF-IDF terms:\n",
      "  green                0.4857\n",
      "  ed                   0.4347\n",
      "  oh                   0.4174\n",
      "  posting              0.4029\n",
      "  thought              0.3480\n",
      "  second               0.3428\n",
      "Topic guess: green | ed | oh | posting | thought\n",
      "\n",
      "=== Document 7  (index 695)  |  Category: sci.med ===\n",
      "Preview: I have heard that epileptic patients go into seizures if they eat anything with MSG added.  This may have something to d...\n",
      "Top 10 TF-IDF terms:\n",
      "  patients             0.5391\n",
      "  msg                  0.4928\n",
      "  added                0.4298\n",
      "  eat                  0.4020\n",
      "  heard                0.3467\n",
      "Topic guess: patients | msg | added | eat | heard\n",
      "\n",
      "=== Document 8  (index 94)  |  Category: comp.graphics ===\n",
      "Preview: There is a program called Graphic Workshop you can FTP from wuarchive.  The file is in the msdos/graphics directory and ...\n",
      "Top 10 TF-IDF terms:\n",
      "  called               0.4158\n",
      "  program              0.4138\n",
      "  graphic              0.3214\n",
      "  wuarchive            0.3133\n",
      "  msdos                0.2895\n",
      "  zip                  0.2850\n",
      "  directory            0.2667\n",
      "  graphics             0.2637\n",
      "  ftp                  0.2608\n",
      "  file                 0.2272\n",
      "Topic guess: called | program | graphic | wuarchive | msdos\n",
      "\n",
      "=== Document 9  (index 201)  |  Category: comp.sys.mac.hardware ===\n",
      "Preview: From: push@media.mit.edu (Pushpinder Singh) Subject: re: Centris 610 Video Problem - I'm having it also! Date: Sat, 17 A...\n",
      "Top 10 TF-IDF terms:\n",
      "  sat                  0.3576\n",
      "  gmt                  0.3454\n",
      "  centris              0.3454\n",
      "  push                 0.3074\n",
      "  mit                  0.2851\n",
      "  apr                  0.2783\n",
      "  date                 0.2641\n",
      "  media                0.2641\n",
      "  subject              0.2550\n",
      "  video                0.2419\n",
      "Topic guess: sat | gmt | centris | push | mit\n",
      "\n",
      "=== Document 10  (index 855)  |  Category: talk.politics.mideast ===\n",
      "Preview: David posts a good translation of a post by Suat Kinikliouglu:  [most of the original post elided]     [KK] ***** VATAN ...\n",
      "Top 10 TF-IDF terms:\n",
      "  translation          0.7445\n",
      "  love                 0.1903\n",
      "  original             0.1766\n",
      "  think                0.1663\n",
      "  post                 0.1590\n",
      "  translate            0.1382\n",
      "  wind                 0.1241\n",
      "  questionable         0.1241\n",
      "  en                   0.1241\n",
      "  soul                 0.1241\n",
      "Topic guess: translation | love | original | think | post\n"
     ]
    }
   ],
   "source": [
    "# Add your code here\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(42)  # reproducible sampling\n",
    "feature_names = np.array(tfidf_vect.get_feature_names_out())\n",
    "\n",
    "# Pick 10 random, distinct documents\n",
    "doc_indices = rng.choice(X_tfidf.shape[0], size=10, replace=False)\n",
    "\n",
    "def top_terms_sparse_row(row_csr, k=10):\n",
    "    \"\"\"\n",
    "    Given a 1 x N sparse CSR row, return top-k (term, score) pairs\n",
    "    without densifying the whole vector.\n",
    "    \"\"\"\n",
    "    data = row_csr.data\n",
    "    idxs = row_csr.indices\n",
    "    if data.size == 0:\n",
    "        return []\n",
    "    top_local = np.argsort(data)[-k:][::-1]\n",
    "    terms = feature_names[idxs[top_local]]\n",
    "    scores = data[top_local]\n",
    "    return list(zip(terms, map(float, scores)))\n",
    "\n",
    "def guess_topic_from_terms(terms, n=5):\n",
    "    \"\"\"Very simple one-line guess from top n terms.\"\"\"\n",
    "    return \" | \".join([t for t, _ in terms[:n]])\n",
    "\n",
    "for i, doc_id in enumerate(doc_indices, start=1):\n",
    "    row = X_tfidf[doc_id]  # 1 x N sparse\n",
    "    top_terms = top_terms_sparse_row(row, k=10)\n",
    "    topic = df_subset.loc[doc_id, \"category_name\"]\n",
    "    preview = df_subset.loc[doc_id, \"content\"][:120].replace(\"\\n\", \" \")\n",
    "\n",
    "    print(f\"\\n=== Document {i}  (index {doc_id})  |  Category: {topic} ===\")\n",
    "    print(f\"Preview: {preview}...\")\n",
    "    print(\"Top 10 TF-IDF terms:\")\n",
    "    for term, score in top_terms:\n",
    "        print(f\"  {term:<20s} {score:.4f}\")\n",
    "    print(\"Topic guess:\", guess_topic_from_terms(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4728f-ee68-43d6-9874-82394438f400",
   "metadata": {},
   "source": [
    "Add your writing here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
